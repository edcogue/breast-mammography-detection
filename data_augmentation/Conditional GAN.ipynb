{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b381f564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3557 11003\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e530e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_channels = 1\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "latent_dim = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9efcc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images: (70000, 28, 28, 1)\n",
      "Shape of training labels: (70000, 10)\n"
     ]
    }
   ],
   "source": [
    "# We'll use all the available examples from both the training and test\n",
    "# sets.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "all_digits = np.concatenate([x_train, x_test])\n",
    "all_labels = np.concatenate([y_train, y_test])\n",
    "\n",
    "# Scale the pixel values to [0, 1] range, add a channel dimension to\n",
    "# the images, and one-hot encode the labels.\n",
    "all_digits = all_digits.astype(\"float32\") / 255.0\n",
    "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
    "all_labels = tf.keras.utils.to_categorical(all_labels, 10)\n",
    "\n",
    "# Create tf.data.Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "print(f\"Shape of training images: {all_digits.shape}\")\n",
    "print(f\"Shape of training labels: {all_labels.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7286653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 11\n"
     ]
    }
   ],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = num_channels + num_classes\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b51697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.InputLayer((28, 28, discriminator_in_channels)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.GlobalMaxPooling2D(),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "# Create the generator.\n",
    "generator = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.InputLayer((generator_in_channels,)),\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "        # 7x7x(128 + num_classes) map.\n",
    "        tf.keras.layers.Dense(7 * 7 * generator_in_channels),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Reshape((7, 7, generator_in_channels)),\n",
    "        tf.keras.layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a04f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ecffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c96d0f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:24:38.923982: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [70000,28,28,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-25 21:24:38.924330: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [70000,10]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-05-25 21:24:41.002238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-05-25 21:24:42.346810: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f571bd18a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-25 21:24:42.346871: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2023-05-25 21:24:42.352707: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-25 21:24:42.473108: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094/1094 [==============================] - 35s 27ms/step - g_loss: 1.3640 - d_loss: 0.4593\n",
      "Epoch 2/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 1.3143 - d_loss: 0.5008\n",
      "Epoch 3/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 1.5120 - d_loss: 0.4085\n",
      "Epoch 4/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 2.0051 - d_loss: 0.2769\n",
      "Epoch 5/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 2.5836 - d_loss: 0.1893\n",
      "Epoch 6/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 1.2916 - d_loss: 0.5383\n",
      "Epoch 7/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 1.0095 - d_loss: 0.6232\n",
      "Epoch 8/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 0.8931 - d_loss: 0.6558\n",
      "Epoch 9/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 0.8130 - d_loss: 0.6823\n",
      "Epoch 10/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 0.7893 - d_loss: 0.6815\n",
      "Epoch 11/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 0.7677 - d_loss: 0.6811\n",
      "Epoch 12/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 0.7749 - d_loss: 0.6771\n",
      "Epoch 13/20\n",
      "1094/1094 [==============================] - 29s 26ms/step - g_loss: 0.7575 - d_loss: 0.6808\n",
      "Epoch 14/20\n",
      "1094/1094 [==============================] - 29s 26ms/step - g_loss: 0.7591 - d_loss: 0.6802\n",
      "Epoch 15/20\n",
      "1094/1094 [==============================] - 29s 26ms/step - g_loss: 0.7585 - d_loss: 0.6765\n",
      "Epoch 16/20\n",
      "1094/1094 [==============================] - 29s 26ms/step - g_loss: 0.7577 - d_loss: 0.6747\n",
      "Epoch 17/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 0.7663 - d_loss: 0.6713\n",
      "Epoch 18/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 0.7764 - d_loss: 0.6698\n",
      "Epoch 19/20\n",
      "1094/1094 [==============================] - 28s 26ms/step - g_loss: 0.7846 - d_loss: 0.6620\n",
      "Epoch 20/20\n",
      "1094/1094 [==============================] - 29s 26ms/step - g_loss: 0.8027 - d_loss: 0.6501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5b443faf70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_gan.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e306148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f57ff144520>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We first extract the trained generator from our Conditiona GAN.\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "def generate_sample(label_ex):\n",
    "    interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
    "    label = tf.keras.utils.to_categorical([label_ex], num_classes)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    noise_and_label = tf.concat([interpolation_noise, label], 1)\n",
    "    fake = trained_gen.predict(noise_and_label)\n",
    "    fake=fake*255\n",
    "    fake=fake.astype(np.uint8)\n",
    "    return fake*255\n",
    "\n",
    "plt.imshow(generate_sample(9)[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5fbcc835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 21:45:27.974458: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?,138]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-25 21:45:27.992418: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?,128]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-25 21:45:28.095071: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,6762]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-25 21:45:28.282512: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,6762]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-25 21:45:28.298181: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?,138]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-25 21:45:28.325392: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,?,128]\n",
      "\t [[{{node inputs}}]]\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: test_gan/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: test_gan/assets\n"
     ]
    }
   ],
   "source": [
    "trained_gen.save(\"test_gan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
